{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).\n",
    "\n",
    "Ensemble methods can be divided into two groups:\n",
    "1. sequential ensemble methods where the base learners are generated sequentially (e.g. AdaBoost).\n",
    "    -The basic motivation of sequential methods is to exploit the dependence between the base learners. The overall performance can be boosted by weighing previously mislabeled examples with higher weight.\n",
    "\n",
    "2. parallel ensemble methods where the base learners are generated in parallel (e.g. Random Forest).\n",
    "    -The basic motivation of parallel methods is to exploit independence between the base learners since the error can be reduced dramatically by averaging.\n",
    "\n",
    "### Types of ensemble learning\n",
    "1. Bagging\n",
    "    - Bagging stands for bootstrap aggregation. One way to reduce the variance of an estimate is to average together multiple estimates. For example, we can train M different trees on different subsets of the data (chosen randomly with replacement) and compute the ensemble. Bagging uses bootstrap sampling to obtain the data subsets for training the base learners. For aggregating the outputs of base learners, bagging uses voting for classification and averaging for regression.\n",
    "    - used in Random Forests\n",
    "    \n",
    "2. Boosting\n",
    "    - Boosting refers to a family of algorithms that are able to convert weak learners to strong learners. The main principle of boosting is to fit a sequence of weak learners− models that are only slightly better than random guessing, such as small decision trees− to weighted versions of the data. More weight is given to examples that were misclassified by earlier rounds.\n",
    "    - The predictions are then combined through a weighted majority vote (classification) or a weighted sum (regression) to produce the final prediction. The principal difference between boosting and the committee methods, such as bagging, is that base learners are trained in sequence on a weighted version of the data.\n",
    "    - used in AdaBoost and Gradient Boosting\n",
    "    \n",
    "3. Stacking\n",
    "    - Stacking is an ensemble learning technique that combines multiple classification or regression models via a meta-classifier or a meta-regressor. The base level models are trained based on a complete training set, then the meta-model is trained on the outputs of the base level model as features.\n",
    "    \n",
    "Reference: https://blog.statsbot.co/ensemble-learning-d1dcd548e936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "kbtDblUdHm8g"
   },
   "source": [
    "## What is Gradient Boosting Machine (GBM)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "TZdLWviIPRRZ"
   },
   "source": [
    "Gradient boosting refers to a class of ensemble machine learning algorithms that can be used for classification or regression predictive modeling problems.\n",
    "\n",
    "Ensembles are constructed from decision tree models. Trees are added one at a time to the ensemble and fit to correct the prediction errors made by prior models. This is a type of ensemble machine learning model referred to as boosting.\n",
    "\n",
    "Boosting combines a set of weak learners and delivers improved prediction accuracy. At any instant t, the model outcomes are weighed based on the outcomes of previous instant t-1. The outcomes predicted correctly are given a lower weight and the ones miss-classified are weighted higher. This technique is followed for a classification problem while a similar technique is used for regression.\n",
    "\n",
    "![alt text](https://www.analyticsvidhya.com/wp-content/uploads/2016/02/boosting.png)\n",
    "\n",
    "Observations:\n",
    "\n",
    "- Box 1: Output of First Weak Learner (From the left)\n",
    "    - Initially all points have same weight (denoted by their size).\n",
    "    - The decision boundary predicts 2 +ve and 5 -ve points correctly.\n",
    "\n",
    "- Box 2: Output of Second Weak Learner\n",
    "    - The points classified correctly in box 1 are given a lower weight and vice versa.\n",
    "    - The model focuses on high weight points now and classifies them correctly. But, others are misclassified now.\n",
    "\n",
    "Similar trend can be seen in box 3 as well. This continues for many iterations. In the end, all models are given a weight depending on their accuracy and a consolidated result is generated.\n",
    "\n",
    "Models are fit using any arbitrary differentiable loss function and gradient descent optimization algorithm. This gives the technique its name, “gradient boosting,” as the loss gradient is minimized as the model is fit, much like a neural network.\n",
    "\n",
    "\n",
    "Reference: \n",
    "- https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/\n",
    "\n",
    "- https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "wzwAh_TQiYtw"
   },
   "source": [
    "## Theory behind GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "jfvkqGediaNA"
   },
   "source": [
    "Gradient boosting involves three elements:\n",
    "\n",
    "1. A loss function to be optimized.\n",
    "2. A weak learner to make predictions.\n",
    "3. An additive model to add weak learners to minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "R6_OYoFRlHE_"
   },
   "source": [
    "### 1. Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "kJzYxUfNk2YB"
   },
   "source": [
    "\n",
    "\n",
    "The loss function used depends on the type of problem being solved. It must be differentiable, but many standard loss functions are supported and you can define your own.\n",
    "\n",
    "For example, regression may use a squared error and classification may use logarithmic loss.\n",
    "\n",
    "A benefit of the gradient boosting framework is that a new boosting algorithm does not have to be derived for each loss function that may want to be used, instead, it is a generic enough framework that any differentiable loss function can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "Fx-WzRyclIL5"
   },
   "source": [
    "### 2. Weak Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "MVNkU8l6lIul"
   },
   "source": [
    "Decision trees are used as the weak learner in gradient boosting. \n",
    "\n",
    "Specifically, regression trees are used that output real values for splits and whose output can be added together, allowing subsequent models outputs to be added and “correct” the residuals in the predictions.\n",
    "\n",
    "Trees are constructed in a greedy manner, choosing the best split points based on purity scores like Gini or to minimize the loss.\n",
    "\n",
    "Initially, such as in the case of AdaBoost, very short decision trees were used that only had a single split, called a decision stump. Larger trees can be used generally with 4-to-8 levels.\n",
    "\n",
    "It is common to constrain the weak learners in specific ways, such as a maximum number of layers, nodes, splits or leaf nodes.\n",
    "\n",
    "This is to ensure that the learners remain weak, but can still be constructed in a greedy manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "kMHzSubelfsr"
   },
   "source": [
    "### 3. Additive Model\n",
    "\n",
    "Trees are added one at a time, and existing trees in the model are not changed.\n",
    "\n",
    "A gradient descent procedure is used to minimize the loss when adding trees.\n",
    "\n",
    "Traditionally, gradient descent is used to minimize a set of parameters, such as the coefficients in a regression equation or weights in a neural network. After calculating error or loss, the weights are updated to minimize that error.\n",
    "\n",
    "Instead of parameters, we have weak learner sub-models or more specifically decision trees. After calculating the loss, to perform the gradient descent procedure, we must add a tree to the model that reduces the loss (i.e. follow the gradient). We do this by parameterizing the tree, then modify the parameters of the tree and move in the right direction by (reducing the residual loss.\n",
    "\n",
    "The output for the new tree is then added to the output of the existing sequence of trees in an effort to correct or improve the final output of the model.\n",
    "\n",
    "A fixed number of trees are added or training stops once loss reaches an acceptable level or no longer improves on an external validation dataset.\n",
    "\n",
    "\n",
    "Reference: https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "I0vNWWqUG31Y"
   },
   "source": [
    "## GBM for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "cEu3NrJFCGi9"
   },
   "source": [
    "Reference: https://www.youtube.com/watch?v=jxuNLH5dXCs\n",
    "\n",
    "1\n",
    "![alt text](https://drive.google.com/uc?id=1DwkXcHG_OKF84GlVSNZ62QAc5Ac73tnV)\n",
    "We will be using dataset on the left and fiting the dataset into a model by Gradient Boost.\n",
    "\n",
    "2\n",
    "![alt text](https://drive.google.com/uc?id=1-EIPLD04kyEwUrEykIRrbHrVoD_sICSr)\n",
    "First, we need to come out with the initial prediction. The initial prediction for Gradient Boost for classifcation is log(odds).\n",
    "\n",
    "3\n",
    "![alt text](https://drive.google.com/uc?id=1ixmUZXQ3kdiVEfDU046fH2WthYCfCvdf)\n",
    "We will convert the log(odds) to probability using the formula shown above.\n",
    "\n",
    "4\n",
    "![alt text](https://drive.google.com/uc?id=1GgPFQw4AFFG2OdtlrxA0FoRrtXfD1wRl)\n",
    "\n",
    "5\n",
    "![alt text](https://drive.google.com/uc?id=16Szo5wJVLbj_FmZGJKqq0VNtL-urH6bG)\n",
    "\n",
    "6\n",
    "![alt text](https://drive.google.com/uc?id=16dt9z89zKzxetF5UsagO9Jzh6PfHVGXE)\n",
    "\n",
    "7\n",
    "![alt text](https://drive.google.com/uc?id=1OqGdTn2KALKJ1IUO1wDscVFSdzY4SKsI)\n",
    "We can measure the residual (aka loss) using the formula: observed value minus predicted value\n",
    "\n",
    "8\n",
    "![alt text](https://drive.google.com/uc?id=1DIIeuT8tLtZz8-oLcCTpGaqNfW7UDL7S)\n",
    "\n",
    "9\n",
    "![alt text](https://drive.google.com/uc?id=1N4Lz5tJtCBFLpjGLy_uokq3t3R7C7VQN)\n",
    "\n",
    "10\n",
    "![alt text](https://drive.google.com/uc?id=1mvxyJvFpynJpRLTnF9fSSE78OIJPX5Kf)\n",
    "\n",
    "11\n",
    "![alt text](https://drive.google.com/uc?id=1tIXFbOYblAHgdTfcDDzI7KhfWNmNvQi0)\n",
    "Now, we will build a decision tree based on the features. The tree can be built by calculating the Gini index for the impurity score.\n",
    "\n",
    "12\n",
    "![alt text](https://drive.google.com/uc?id=1DCAKIIuBNEEQtWLEen_Mx12Yt9oMXkvU)\n",
    "\n",
    "13\n",
    "![alt text](https://drive.google.com/uc?id=1AiiCpu7U1CUd3XJPEgmQxHd3QEhWKUjP)\n",
    "\n",
    "14\n",
    "![alt text](https://drive.google.com/uc?id=1zuDXudXPu5JTcXi2-yesOjHngVd8PTgY)\n",
    "We are having problem with the terms to update our prediction as the initial prediction is in log(odds) but the residual is derived from probability.\n",
    "\n",
    "15\n",
    "![alt text](https://drive.google.com/uc?id=1FzByZYqsI1v8qV61Kdq_mE2sX4kn1DUe)\n",
    "\n",
    "16\n",
    "![alt text](https://drive.google.com/uc?id=1ve3USn-q71o-fDPszkdcJv0C3J6T-Sym)\n",
    "Thus, we will use the formula shown above to do transformation on the residual and make it in terms of log(odds).\n",
    "\n",
    "17\n",
    "![alt text](https://drive.google.com/uc?id=1tW3evkHenB6JMYcRhqQWpar-43WLEPzI)\n",
    "\n",
    "18\n",
    "![alt text](https://drive.google.com/uc?id=12IzkB7MiMpUpfcW2jswSoVdPMfuyvevV)\n",
    "\n",
    "19\n",
    "![alt text](https://drive.google.com/uc?id=1ANPEUBUI5rV6gTnQe-beeQnpzbfA6c9P)\n",
    "We will update the prediction with the output value multipled by a learning rate (0.8 as shown above, but 0.1 will be more common)\n",
    "\n",
    "20\n",
    "![alt text](https://drive.google.com/uc?id=1Go8SixnQJC-lJmXebKmIOewmYh29S1NI)\n",
    "We convert the new log(odds) prediction into probabilitiy and calculate the new residual.\n",
    "\n",
    "21\n",
    "![alt text](https://drive.google.com/uc?id=1i85IJTp-x6EyBJGMNb6owZFSlkQevj8v)\n",
    "\n",
    "22\n",
    "![alt text](https://drive.google.com/uc?id=1h0V-ouEfTtVGgsMHrlysUyAbTJHTF6ns)\n",
    "\n",
    "23\n",
    "![alt text](https://drive.google.com/uc?id=1gpA173UULMUKQXtq3wq9aOsR2TclJPdm)\n",
    "We will be doing the transformation again (using the formula shown before) to ensure the new prediction is in term of log(odds). Note that the new predicted value is different for each row now.\n",
    "\n",
    "24\n",
    "![alt text](https://drive.google.com/uc?id=1U2ZVAlOzByprCcB99i2r-xNa3AxVyY8F)\n",
    "\n",
    "25\n",
    "![alt text](https://drive.google.com/uc?id=1Nfwm1RJdnuW3Lz9iP77WBYh9Z-IOHact)\n",
    "\n",
    "26\n",
    "![alt text](https://drive.google.com/uc?id=1M__QJ688hpzBi2h54K9BrMBXhsh5zUs5)\n",
    "From here, we can summarise what we have gone through so far\n",
    "\n",
    "27\n",
    "![alt text](https://drive.google.com/uc?id=1UMkpUi5VCkKFSb2PbkW2gaxyzeLGBVzH)\n",
    "\n",
    "28\n",
    "![alt text](https://drive.google.com/uc?id=14Z1n3VOQK9JEfwEZ5eyNl8gfDzwaq9oh)\n",
    "\n",
    "29\n",
    "![alt text](https://drive.google.com/uc?id=1G8L36BRTASeWNs3rZ8m0HahQ6myugd3n)\n",
    "\n",
    "30\n",
    "![alt text](https://drive.google.com/uc?id=1ogyvlbfBPGNfTT3ldzBs_w3PNMCilix7)\n",
    "\n",
    "31\n",
    "![alt text](https://drive.google.com/uc?id=1k7BMtvejrfkXLwGGznp7odwRr4mAx-Ha)\n",
    "Now, we will try to classify the sample on the left and see whether the person loves the movie.\n",
    "\n",
    "32\n",
    "![alt text](https://drive.google.com/uc?id=12hSnsxm9565lvS01jRygMWcwzc6MioJe)\n",
    "We can calculate the log(odds) prediction for that sample by going down the decision tress and the feature of the sample.\n",
    "\n",
    "33\n",
    "![alt text](https://drive.google.com/uc?id=1XKKs2Gn0WCFf7GugVXXKRr1mKpS4VVNI)\n",
    "\n",
    "34\n",
    "![alt text](https://drive.google.com/uc?id=1BnrwjZFPsWjvHtHMUhvF6eCwYP1CObJm)\n",
    "Finally, we can convert the log(odds) prediction into probability and see whether the person loves the movie. In this case, since 0.9 > 0.5, we can conclude that he/she loves the movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "5a-MuTf4HnxZ"
   },
   "source": [
    "## GBM Parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "tTsKSi1BHwSV"
   },
   "source": [
    "The overall parameters of this ensemble model can be divided into 3 categories:\n",
    "\n",
    "1. Tree-Specific Parameters: These affect each individual tree in the model.\n",
    "\n",
    "2. Boosting Parameters: These affect the boosting operation in the model.\n",
    "\n",
    "3. Miscellaneous Parameters: Other parameters for overall functioning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "VCt7gIqsIHSM"
   },
   "source": [
    "### 1. Tree-Specific Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "n9ejkth0INlq"
   },
   "source": [
    "![alt text](https://www.analyticsvidhya.com/wp-content/uploads/2016/02/tree-infographic.png)\n",
    "\n",
    "1. min_samples_split\n",
    "    \n",
    "    - Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting.    \n",
    "    \n",
    "    - Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    \n",
    "    - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "\n",
    "\n",
    "2. min_samples_leaf\n",
    "   \n",
    "    - Defines the minimum samples (or observations) required in a terminal node or leaf.   \n",
    "    - Used to control over-fitting similar to min_samples_split.   \n",
    "    - Generally lower values should be chosen for imbalanced class problems because the regions in which the minority class will be in majority will be very small.\n",
    "\n",
    "\n",
    "3. min_weight_fraction_leaf\n",
    "\n",
    "  - Similar to min_samples_leaf but defined as a fraction of the total number of observations instead of an integer.\n",
    "  - Only one of #2 and #3 should be defined.\n",
    "\n",
    "\n",
    "4. max_depth\n",
    "\n",
    "  - The maximum depth of a tree.\n",
    "  - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "  - Should be tuned using CV.\n",
    "\n",
    "\n",
    "5. max_leaf_nodes\n",
    "\n",
    "  - The maximum number of terminal nodes or leaves in a tree.\n",
    "  - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "  - If this is defined, GBM will ignore max_depth.\n",
    "\n",
    "\n",
    "6. max_features\n",
    "\n",
    "  - The number of features to consider while searching for a best split. These will be randomly selected.\n",
    "  - As a thumb-rule, square root of the total number of features works great but we should check upto 30-40% of the total number of features.\n",
    "  - Higher values can lead to over-fitting but depends on case to case.\n",
    "\n",
    "P/s: In the scikit-learn user guide under the section titled “Gradient Tree Boosting” the authors comment that setting the maximum leaf nodes has a similar effect to setting the max depth to the maximum leaf nodes minus one, but results in worse performance.\n",
    "\n",
    "- We found that max_leaf_nodes=k gives comparable results to max_depth=k-1 but is significantly faster to train at the expense of a slightly higher training error.\n",
    "\n",
    "The parameters which we have considered so far will affect how the model (eg: decision trees) is being built. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "9rJ-62bXLLe5"
   },
   "source": [
    "### 2. Boosting Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "iP1EOkODLr_-"
   },
   "source": [
    "1. learning_rate\n",
    "\n",
    "  - This determines the impact of each tree on the final outcome. GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates.\n",
    "  - Lower values are generally preferred as they make the model robust to the specific characteristics of tree and thus allowing it to generalize well.\n",
    "  - Lower values would require higher number of trees to model all the relations and will be computationally expensive.\n",
    "\n",
    "\n",
    "2. n_estimators\n",
    "  - The number of sequential (decision) trees to be modeled \n",
    "  - Though GBM is fairly robust at higher number of trees but it can still overfit at a point. Hence, this should be tuned using CV for a particular learning rate.\n",
    "\n",
    "\n",
    "3. subsample\n",
    "  - The fraction of observations to be selected for each tree. Selection is done by random sampling.\n",
    "  - Values slightly less than 1 make the model robust by reducing the variance.\n",
    "  - Typical values ~0.8 generally work fine but can be fine-tuned further.\n",
    "\n",
    "This set of parameters is used for managing how the boosting process is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "ECuf4-rvMWVv"
   },
   "source": [
    "### 3. Miscellaneous Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "OWktMR96Mu1f"
   },
   "source": [
    "Apart from these, there are certain miscellaneous parameters which affect overall functionality:\n",
    "\n",
    "1. loss\n",
    "  - It refers to the loss function to be minimized in each split.\n",
    "  - It can have various values for classification and regression case. Generally the default values work fine. Other values should be chosen only if you understand their impact on the model.\n",
    "\n",
    "\n",
    "2. init\n",
    "  - This affects initialization of the output.\n",
    "  - This can be used if we have made another model whose outcome is to be used as the initial estimates for GBM.\n",
    "\n",
    "\n",
    "3. random_state\n",
    "  - The random number seed so that same random numbers are generated every time.\n",
    "  - This is important for parameter tuning. If we don’t fix the random number, then we’ll have different outcomes for subsequent runs on the same parameters and it becomes difficult to compare models.\n",
    "  - It can potentially result in overfitting to a particular random sample selected. We can try running models for different random samples, which is computationally expensive and generally not used.\n",
    "\n",
    "\n",
    "4. verbose\n",
    "  - The type of output to be printed when the model fits. The different values can be:\n",
    "    - 0: no output generated (default)\n",
    "    - 1: output generated for trees in certain intervals\n",
    "    - more than 1: output generated for all trees\n",
    "\n",
    "\n",
    "5. warm_start\n",
    "  - This parameter has an interesting application and can help a lot if used judicially.\n",
    "  - Using this, we can fit additional trees on previous fits of a model. It can save a lot of time and you should explore this option for advanced applications\n",
    "\n",
    "\n",
    "6. presort \n",
    "  - Select whether to presort data for faster splits.\n",
    "  - It makes the selection automatically by default but it can be changed if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "jHv09AVzPElR"
   },
   "source": [
    "### Summary for GBM parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "GHHXLVUqOMzH"
   },
   "source": [
    "![alt text](https://drive.google.com/uc?id=1y2kWrZuzBS9dGbB8V4Oc-Vagi5WgIMn2)\n",
    "![alt text](https://drive.google.com/uc?id=1Vcb4-Lvjmga2yJHO6IoCj1KqKiir_9FZ)\n",
    "\n",
    "Reference: \n",
    "- https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "- https://machinelearningmastery.com/configure-gradient-boosting-algorithm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Strategy in tuning the parameters (Own assessment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In practice, we would not necessary to tune all the parameters shown above in GBM as it would be time consuming and computationally expensive. Thus, here is some strategies to choose which parameters to tune based on my own assessment:\n",
    "\n",
    "1. learning_rate and n_estimators\n",
    "    - learning_rate would affect the impact of each weak leaners (decision trees) towards the final model. Usually, we would prefer a lower value as it could make the model captures the specific characteristics of tree and thus allowing it to generalize well.\n",
    "    - n_estimators decide the number of decision trees. Although GBM is more accurate and stronger with higher number of trees but it can overfit at a point.\n",
    "    - usually we will tune both together as there is tradeoff between two parameters. We want to learning_rate to be low and  set the n_estimators to be large ideally, but it could be computationally expensive and time consuming to look for the best pair\n",
    "\n",
    "\n",
    "2. subsample\n",
    "    - subsample will decide the fraction of observations to be selected for each tree and selection is done by random sampling.\n",
    "    - values slightly less than 1 will make make the model stronger by reducing the variance.\n",
    "    - typically we will use 0.8 but it could be fine tuned further (default value is 1)\n",
    "\n",
    "\n",
    "3. min_samples_spilt and min_samples_leaf\n",
    "    - min_samples_spilt determines the minimum number of samples to spilt an internal node, usually we will choose value equal to 0.5% - 2% of total number of samples\n",
    "    - min_samples_leaf determines the minimum number of samples required in a leaf node for the spilt to be valid\n",
    "    - both of these parameter behave in a similar way, too high value can cause underfitting, but lower value can cause overfitting too. Thus, we will fine tune it with CV to look for the optimal value for both, basically trial and error\n",
    "\n",
    "\n",
    "4. max_depth\n",
    "    - max_depth will decide the maximum depth of each tree. For example, a max depth of 5 will result in 32 leaf nodes as 2^5 = 32\n",
    "    - the typical value for max_depth is between 5 to 20, again, too high value can cause overfitting and vice versa. Thus, need to fine tune it with CV to look for the optimal value.\n",
    " \n",
    " \n",
    "5. max_features\n",
    "    - max_features determines the number of features to consider while searching for a best split. The features will be randomly selected.\n",
    "    - As a thumb-rule, square root ('sqrt') of the total number of features works great but we should check upto 30-40% of the total number of features.\n",
    "    \n",
    "In a nutshell, these are the main parameters you should take note when fine-tuning GBM. Most of the times, you might need to trial and error to see which value optimizes your result the most, there is just no one hard and fast rule for every problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "WlmlQbpGuGgm"
   },
   "source": [
    "## 1. IMDB movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "6hiCqXlhESyl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "hidden": true,
    "id": "Xh150xfiaarC",
    "outputId": "800dcded-f8c4-448a-e46f-38c57dd38da4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "hidden": true,
    "id": "ZYiKIkO2adH6",
    "outputId": "bef8c5e2-d930-42ed-bef9-98e33b06f169"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10002_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10003_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>99995</td>\n",
       "      <td>train</td>\n",
       "      <td>Delightfully awful! Made by David Giancola, a ...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9998_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>99996</td>\n",
       "      <td>train</td>\n",
       "      <td>Watching Time Chasers, it obvious that it was ...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9999_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>99997</td>\n",
       "      <td>train</td>\n",
       "      <td>At the beginning we can see members of Troma t...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>999_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>99998</td>\n",
       "      <td>train</td>\n",
       "      <td>The movie was incredible, ever since I saw it ...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>99_0.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>99999</td>\n",
       "      <td>train</td>\n",
       "      <td>TCM came through by acquiring this wonderful, ...</td>\n",
       "      <td>unsup</td>\n",
       "      <td>9_0.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0   type  ...  label         file\n",
       "0               0   test  ...    neg      0_2.txt\n",
       "1               1   test  ...    neg  10000_4.txt\n",
       "2               2   test  ...    neg  10001_1.txt\n",
       "3               3   test  ...    neg  10002_3.txt\n",
       "4               4   test  ...    neg  10003_3.txt\n",
       "...           ...    ...  ...    ...          ...\n",
       "99995       99995  train  ...  unsup   9998_0.txt\n",
       "99996       99996  train  ...  unsup   9999_0.txt\n",
       "99997       99997  train  ...  unsup    999_0.txt\n",
       "99998       99998  train  ...  unsup     99_0.txt\n",
       "99999       99999  train  ...  unsup      9_0.txt\n",
       "\n",
       "[100000 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df = pd.read_csv('/content/gdrive/My Drive/Dataset/imdb.csv', \n",
    "                      encoding='iso-8859-1')\n",
    "imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "hidden": true,
    "id": "waByw5BCaixg",
    "outputId": "f022b426-f3cb-45c3-a1f4-5a4fe234df7f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>Story of a man who has unnatural feelings for ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25001</th>\n",
       "      <td>Airport '77 starts as a brand new luxury 747 p...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>This film lacked something I couldn't put my f...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>Sorry everyone,,, I know this is supposed to b...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>When I was little my parents took me along to ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>Seeing as the vote average was pretty low, and...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The plot had some wretched, unbelievable twist...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am amazed at how this movie(and most others ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>A Christmas Together actually came before my t...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>Working-class romantic drama from director Mar...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review label\n",
       "25000  Story of a man who has unnatural feelings for ...   neg\n",
       "25001  Airport '77 starts as a brand new luxury 747 p...   neg\n",
       "25002  This film lacked something I couldn't put my f...   neg\n",
       "25003  Sorry everyone,,, I know this is supposed to b...   neg\n",
       "25004  When I was little my parents took me along to ...   neg\n",
       "...                                                  ...   ...\n",
       "49995  Seeing as the vote average was pretty low, and...   pos\n",
       "49996  The plot had some wretched, unbelievable twist...   pos\n",
       "49997  I am amazed at how this movie(and most others ...   pos\n",
       "49998  A Christmas Together actually came before my t...   pos\n",
       "49999  Working-class romantic drama from director Mar...   pos\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_df = imdb_df[(imdb_df['type'] == 'train') & (imdb_df['label'] != 'unsup')]\n",
    "imdb_df = imdb_df.filter(['review','label'])\n",
    "imdb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "hidden": true,
    "id": "B-edPTw2apBd",
    "outputId": "03ed2637-1678-4bca-be34-9f80e2f0d903"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 2 artists>"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(imdb_df['label'].unique(),imdb_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "hidden": true,
    "id": "3WwVeWzXayv4",
    "outputId": "3b2428d5-ec8b-441d-b15c-26c74f42bddb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25001</th>\n",
       "      <td>airport  starts as a brand new luxury  plane i...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>this film lacked something i couldnt put my fi...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>sorry everyone i know this is supposed to be a...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>when i was little my parents took me along to ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review label\n",
       "25000  story of a man who has unnatural feelings for ...   neg\n",
       "25001  airport  starts as a brand new luxury  plane i...   neg\n",
       "25002  this film lacked something i couldnt put my fi...   neg\n",
       "25003  sorry everyone i know this is supposed to be a...   neg\n",
       "25004  when i was little my parents took me along to ...   neg"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    text = ''.join(i for i in text if ord(i) < 128)\n",
    "    return text\n",
    "\n",
    "imdb_df['review'] = imdb_df['review'].apply(lambda x: clean(x))\n",
    "imdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "YEk_qb0qbI4a"
   },
   "outputs": [],
   "source": [
    "#using tf-idf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_vectorizer.fit(imdb_df[\"review\"])\n",
    "X = tfidf_vectorizer.transform(imdb_df[\"review\"])\n",
    "y = imdb_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "hidden": true,
    "id": "9YebGsa8beQT",
    "outputId": "efe1a8fe-6158-4a9c-baef-cd2a7bb87641"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the GBM on test set: 0.801\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.85      0.74      0.79      2493\n",
      "         pos       0.77      0.87      0.81      2507\n",
      "\n",
      "    accuracy                           0.80      5000\n",
      "   macro avg       0.81      0.80      0.80      5000\n",
      "weighted avg       0.81      0.80      0.80      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "baseline = GradientBoostingClassifier()\n",
    "baseline.fit(X_train,y_train)\n",
    "print('Accuracy of the GBM on test set: {:.3f}'.format(baseline.score(X_test, y_test)))\n",
    "pred=baseline.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "hidden": true,
    "id": "k3w4QKF3usVQ",
    "outputId": "52b44be2-7b76-42d2-ce49-660ec9780c2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Logistic Regression on test set: 0.880\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.89      0.86      0.88      2493\n",
      "         pos       0.87      0.90      0.88      2507\n",
      "\n",
      "    accuracy                           0.88      5000\n",
      "   macro avg       0.88      0.88      0.88      5000\n",
      "weighted avg       0.88      0.88      0.88      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "baseline = LogisticRegression()\n",
    "baseline.fit(X_train,y_train)\n",
    "print('Accuracy of the Logistic Regression on test set: {:.3f}'.format(baseline.score(X_test, y_test)))\n",
    "pred=baseline.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "M70s1nvALmnJ",
    "outputId": "dec08097-89e8-46f2-bb21-a0e95999e570"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'learning_rate': 0.1, 'n_estimators': 1000}, 0.86805)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test1 = {'learning_rate':[0.1,0.01,0.001], 'n_estimators':[100,500,1000]}\n",
    "\n",
    "tuning = GridSearchCV(estimator =GradientBoostingClassifier(max_depth=4, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10), \n",
    "            param_grid = p_test1, scoring='accuracy')\n",
    "tuning.fit(X_train,y_train)\n",
    "tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "eWiERcseb-_w",
    "outputId": "70c02a4b-379e-4898-9be8-21c862f5ca69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 4}, 0.86805)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test2 = {'max_depth':[2,3,4] }\n",
    "tuning = GridSearchCV(estimator =GradientBoostingClassifier(learning_rate=0.1,n_estimators=1000, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10), \n",
    "            param_grid = p_test2, scoring='accuracy')\n",
    "tuning.fit(X_train,y_train)\n",
    "tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "hidden": true,
    "id": "1gp8nwQ0cDCA",
    "outputId": "39b3b06e-a887-4355-a66c-01b5475df791"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the GBM on test set: 0.880\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.89      0.84      0.86      2493\n",
      "         pos       0.85      0.89      0.87      2507\n",
      "\n",
      "    accuracy                           0.87      5000\n",
      "   macro avg       0.87      0.87      0.87      5000\n",
      "weighted avg       0.87      0.87      0.87      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model1 = GradientBoostingClassifier(learning_rate=0.1, n_estimators=1000,max_depth=4, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10)\n",
    "model1.fit(X_train,y_train)\n",
    "print('Accuracy of the GBM on test set: {:.3f}'.format(baseline.score(X_test, y_test)))\n",
    "pred=model1.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "UVqWLKyYcG1w",
    "outputId": "a4b28f5d-1fd9-445c-9a17-45a4c454bd29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'min_samples_leaf': 3, 'min_samples_split': 20}, 0.86865)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test3 = {'min_samples_split':[2,10,20], 'min_samples_leaf':[1,3,5]}\n",
    "\n",
    "tuning = GridSearchCV(estimator =GradientBoostingClassifier(learning_rate=0.1, n_estimators=1000,max_depth=4, subsample=1,max_features='sqrt', random_state=10), \n",
    "            param_grid = p_test3, scoring='accuracy')\n",
    "tuning.fit(X_train,y_train)\n",
    "tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "YdsM5PBomjDh",
    "outputId": "422b54fc-3097-4dc1-bb8e-c5c133ac24f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'subsample': 0.8}, 0.8689500000000001)"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test4= {'subsample':[0.8,0.9,1]}\n",
    "\n",
    "tuning = GridSearchCV(estimator =GradientBoostingClassifier(learning_rate=0.1, n_estimators=1000,max_depth=4, min_samples_split=20, min_samples_leaf=3,max_features='sqrt' , random_state=10), \n",
    "          param_grid = p_test4, scoring='accuracy')\n",
    "tuning.fit(X_train,y_train)\n",
    "tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "hidden": true,
    "id": "amILHiQNmmTz",
    "outputId": "35e82d6f-4ca1-4515-9224-ca1a9423cd1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the GBM on test set: 0.866\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.89      0.84      0.86      2493\n",
      "         pos       0.85      0.89      0.87      2507\n",
      "\n",
      "    accuracy                           0.87      5000\n",
      "   macro avg       0.87      0.87      0.87      5000\n",
      "weighted avg       0.87      0.87      0.87      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new=GradientBoostingClassifier(learning_rate=0.1, n_estimators=1000,max_depth=4, min_samples_split=2, min_samples_leaf=1,max_features='sqrt' , subsample=1, random_state=10)\n",
    "new.fit(X_train,y_train)\n",
    "print('Accuracy of the GBM on test set: {:.3f}'.format(new.score(X_test, y_test)))\n",
    "pred=new.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "EcyvqpEvuBlP"
   },
   "source": [
    "## 2. SST2 datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "hidden": true,
    "id": "h49rz5wy3huL",
    "outputId": "2ea14c4f-600a-4983-be03-13b064b9f7fd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>painful , horrifying and oppressively tragic ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>take care is nicely performed by a quintet of ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>the script covers huge , heavy topics in a bla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>a seriously bad film with seriously warped log...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>a deliciously nonsensical comedy about a city ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6920 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  label\n",
       "0     a stirring , funny and finally transporting re...      1\n",
       "1     apparently reassembled from the cutting room f...      0\n",
       "2     they presume their audience wo n't sit still f...      0\n",
       "3     this is a visually stunning rumination on love...      1\n",
       "4     jonathan parker 's bartleby should have been t...      1\n",
       "...                                                 ...    ...\n",
       "6915  painful , horrifying and oppressively tragic ,...      1\n",
       "6916  take care is nicely performed by a quintet of ...      0\n",
       "6917  the script covers huge , heavy topics in a bla...      0\n",
       "6918  a seriously bad film with seriously warped log...      0\n",
       "6919  a deliciously nonsensical comedy about a city ...      1\n",
       "\n",
       "[6920 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', \n",
    "                      delimiter='\\t',header=None)\n",
    "train.columns = ['review','label']\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "hidden": true,
    "id": "ngzJNmNuAdm3",
    "outputId": "41bd2cf7-f817-45f3-c74e-07a5d5e09c7a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no movement , no yuks , not much of anything</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a gob of drivel so sickly sweet , even the eag...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gangs of new york is an unapologetic mess , wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we never really feel involved with the story ,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this is one of polanski 's best films</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>an often deadly boring , strange reading of a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>the problem with concept films is that if the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>safe conduct , however ambitious and well inte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>a film made with as little wit , interest , an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>but here 's the real damn it is n't funny , ei...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1821 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  label\n",
       "0          no movement , no yuks , not much of anything      0\n",
       "1     a gob of drivel so sickly sweet , even the eag...      0\n",
       "2     gangs of new york is an unapologetic mess , wh...      0\n",
       "3     we never really feel involved with the story ,...      0\n",
       "4                 this is one of polanski 's best films      1\n",
       "...                                                 ...    ...\n",
       "1816  an often deadly boring , strange reading of a ...      0\n",
       "1817  the problem with concept films is that if the ...      0\n",
       "1818  safe conduct , however ambitious and well inte...      0\n",
       "1819  a film made with as little wit , interest , an...      0\n",
       "1820  but here 's the real damn it is n't funny , ei...      0\n",
       "\n",
       "[1821 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/test.tsv', \n",
    "                      delimiter='\\t',header=None)\n",
    "test.columns = ['review','label']\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "hidden": true,
    "id": "Bw4wZ3bDA6et",
    "outputId": "d398b743-d642-4491-dc9a-957ae20b376f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>an often deadly boring , strange reading of a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>the problem with concept films is that if the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>safe conduct , however ambitious and well inte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>a film made with as little wit , interest , an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>but here 's the real damn it is n't funny , ei...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8741 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  label\n",
       "0     a stirring , funny and finally transporting re...      1\n",
       "1     apparently reassembled from the cutting room f...      0\n",
       "2     they presume their audience wo n't sit still f...      0\n",
       "3     this is a visually stunning rumination on love...      1\n",
       "4     jonathan parker 's bartleby should have been t...      1\n",
       "...                                                 ...    ...\n",
       "1816  an often deadly boring , strange reading of a ...      0\n",
       "1817  the problem with concept films is that if the ...      0\n",
       "1818  safe conduct , however ambitious and well inte...      0\n",
       "1819  a film made with as little wit , interest , an...      0\n",
       "1820  but here 's the real damn it is n't funny , ei...      0\n",
       "\n",
       "[8741 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sst_df =  train.append(test)\n",
    "sst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "hidden": true,
    "id": "pQMtq4OHtO7K",
    "outputId": "180c1609-b9f6-4036-d9a6-2b080fb3abc3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring  funny and finally transporting re ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo nt sit still fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker s bartleby should have been th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  a stirring  funny and finally transporting re ...      1\n",
       "1  apparently reassembled from the cutting room f...      0\n",
       "2  they presume their audience wo nt sit still fo...      0\n",
       "3  this is a visually stunning rumination on love...      1\n",
       "4  jonathan parker s bartleby should have been th...      1"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    text = ''.join(i for i in text if ord(i) < 128)\n",
    "    return text\n",
    "\n",
    "sst_df['review'] = sst_df['review'].apply(lambda x: clean(x))\n",
    "sst_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "smcVYfxYtY2v"
   },
   "outputs": [],
   "source": [
    "#using tf-idf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_vectorizer.fit(sst_df[\"review\"])\n",
    "X = tfidf_vectorizer.transform(sst_df[\"review\"])\n",
    "y = sst_df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "hidden": true,
    "id": "49Biw76btjgg",
    "outputId": "e18a8b4c-0134-412f-9ab9-5c61b534c23b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the GBM on test set: 0.658\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.47      0.57       833\n",
      "           1       0.63      0.83      0.72       916\n",
      "\n",
      "    accuracy                           0.66      1749\n",
      "   macro avg       0.67      0.65      0.64      1749\n",
      "weighted avg       0.67      0.66      0.65      1749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "baseline = GradientBoostingClassifier()\n",
    "baseline.fit(X_train,y_train)\n",
    "print('Accuracy of the GBM on test set: {:.3f}'.format(baseline.score(X_test, y_test)))\n",
    "pred=baseline.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "hidden": true,
    "id": "dTo0fXVAtlLn",
    "outputId": "d3e5baa2-3a9b-4f27-ba56-4b0e253cce99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Logistic Regression on test set: 0.781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.76      0.77       833\n",
      "           1       0.79      0.80      0.79       916\n",
      "\n",
      "    accuracy                           0.78      1749\n",
      "   macro avg       0.78      0.78      0.78      1749\n",
      "weighted avg       0.78      0.78      0.78      1749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "baseline = LogisticRegression()\n",
    "baseline.fit(X_train,y_train)\n",
    "print('Accuracy of the Logistic Regression on test set: {:.3f}'.format(baseline.score(X_test, y_test)))\n",
    "pred=baseline.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "Ro_9ui8ntoTS",
    "outputId": "3f5f872c-c9eb-4419-efec-d5f0a917ab4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'learning_rate': 0.1, 'n_estimators': 1000}, 0.7339843194761025)"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test1 = {'learning_rate':[0.1,0.01,0.001], 'n_estimators':[100,500,1000]}\n",
    "\n",
    "tuning = GridSearchCV(estimator =GradientBoostingClassifier(), \n",
    "            param_grid = p_test1, scoring='accuracy')\n",
    "tuning.fit(X_train,y_train)\n",
    "tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "IXA_E84Htt6P",
    "outputId": "c8c3f70d-67d2-4265-8d92-d6b780d12d65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 4}, 0.7451390273657558)"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test2 = {'max_depth':[2,3,4] }\n",
    "tuning = GridSearchCV(estimator =GradientBoostingClassifier(learning_rate=0.1,n_estimators=1000), \n",
    "            param_grid = p_test2, scoring='accuracy')\n",
    "tuning.fit(X_train,y_train)\n",
    "tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "hidden": true,
    "id": "sWFDl3w6tzVh",
    "outputId": "9034362a-0180-466c-ba94-1d7f27653fec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the GBM on test set: 0.781\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.75      0.73       833\n",
      "           1       0.76      0.73      0.74       916\n",
      "\n",
      "    accuracy                           0.74      1749\n",
      "   macro avg       0.74      0.74      0.74      1749\n",
      "weighted avg       0.74      0.74      0.74      1749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model1 = GradientBoostingClassifier(learning_rate=0.1, n_estimators=1000,max_depth=4)\n",
    "model1.fit(X_train,y_train)\n",
    "print('Accuracy of the GBM on test set: {:.3f}'.format(baseline.score(X_test, y_test)))\n",
    "pred=model1.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "U2hPsA-2t2Rx",
    "outputId": "eaa14a41-50f4-4de2-d625-b1bc736a4fff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'min_samples_leaf': 1, 'min_samples_split': 2}, 0.7462822923792899)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test3 = {'min_samples_split':[2,10,20], 'min_samples_leaf':[1,3,5]}\n",
    "\n",
    "tuning = GridSearchCV(estimator =GradientBoostingClassifier(learning_rate=0.1, n_estimators=1000,max_depth=4), \n",
    "            param_grid = p_test3, scoring='accuracy')\n",
    "tuning.fit(X_train,y_train)\n",
    "tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "qwQZfllFt8uL",
    "outputId": "ad262cf2-f5dc-4ce3-d0b7-b9707a86b416"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'subsample': 1}, 0.7439948420136598)"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test4= {'subsample':[0.8,0.9,1]}\n",
    "\n",
    "tuning = GridSearchCV(estimator =GradientBoostingClassifier(learning_rate=0.1, n_estimators=1000,max_depth=4, min_samples_split=2, min_samples_leaf=1), \n",
    "          param_grid = p_test4, scoring='accuracy')\n",
    "tuning.fit(X_train,y_train)\n",
    "tuning.best_params_, tuning.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "hidden": true,
    "id": "b_Ojw6wyt_Vm",
    "outputId": "0ec9ecb6-f0e8-40e1-ed2a-e75fa85981b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the GBM on test set: 0.731\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.72       833\n",
      "           1       0.75      0.72      0.74       916\n",
      "\n",
      "    accuracy                           0.73      1749\n",
      "   macro avg       0.73      0.73      0.73      1749\n",
      "weighted avg       0.73      0.73      0.73      1749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new=GradientBoostingClassifier(learning_rate=0.1, n_estimators=1000,max_depth=4, min_samples_split=2, min_samples_leaf=1,max_features='sqrt' , subsample=1)\n",
    "new.fit(X_train,y_train)\n",
    "print('Accuracy of the GBM on test set: {:.3f}'.format(new.score(X_test, y_test)))\n",
    "pred=new.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "kg4NYEIznlCy"
   },
   "source": [
    "Reference:\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#examples-using-sklearn-model-selection-gridsearchcv\n",
    "\n",
    "- https://www.datacareer.ch/blog/parameter-tuning-in-gradient-boosting-gbm-with-python/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Gradient boosting machine.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
